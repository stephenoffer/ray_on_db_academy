{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ray Tune -Ray Tune with Sklearn Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<img src=\"https://docs.ray.io/en/latest/_images/tune_overview.png\" align=\"center\" width=\"50%\">\n",
    "\n",
    "Scikit-Learn is one of the most widely used tools in the ML community for working with data, offering dozens of easy-to-use machine learning algorithms. However, to achieve high performance for these algorithms, you often need to perform **model selection**. Model selection is way to elect the best performant model, after tuning over a set of parameters.\n",
    "\n",
    "`tune-sklearn` is a module that integrates Ray Tune's hyperparameter tuning and scikit-learn's Classifier API. `tune-sklearn` has two APIs: [TuneSearchCV](https://docs.ray.io/en/latest/tune/api_docs/sklearn.html#tunesearchcv-docs) and [TuneGridSearchCV](https://docs.ray.io/en/latest/tune/api_docs/sklearn.html#tunesearchcv-docs). They are drop-in replacements for scikit-learn's [RandomizedSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html) and [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html?highlight=gridsearchcv#sklearn.model_selection.GridSearchCV), so you only need to change less than five lines in a standard scikit-Learn script to use Tune's replacement API.\n",
    "\n",
    "Let's compare Tune's scikit-learn APIs to the standard scikit-learn `GridSearchCV`. For this example, we'll be using `TuneGridSearchCV` with a stochastic gradient descent (SGD) [SGDClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html).\n",
    "\n",
    "To start out, include the import statement to get tune-scikit-learn’s grid search cross validation interface.\n",
    "\n",
    "We need to install a few libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tune-sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.8/site-packages/statsmodels/tsa/base/tsa_model.py:7: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import (to_datetime, Int64Index, DatetimeIndex, Period,\n",
      "/usr/local/anaconda3/lib/python3.8/site-packages/statsmodels/tsa/base/tsa_model.py:7: FutureWarning: pandas.Float64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import (to_datetime, Int64Index, DatetimeIndex, Period,\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "# Import Tune's replacements\n",
    "from ray.tune.sklearn import TuneGridSearchCV\n",
    "from ray.tune.sklearn import TuneSearchCV\n",
    "\n",
    "# Other relevant imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Use the stochastic gradient descent (SGD) classifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "# import the classification dataset\n",
    "from sklearn import datasets\n",
    "from sklearn.datasets import make_classification\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create classification data using `sklearn.datasets`. To start with, with we using a small dataset of 11K rows and 1k columns. As an excercise you can increase the number and see the difference between using regular scikit-learn and tune-scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_classification_data() -> (np.ndarray, np.ndarray):\n",
    "    X, y = make_classification(\n",
    "        n_samples=11000,\n",
    "        n_features=1000,\n",
    "        n_informative=50,\n",
    "        n_redundant=0,\n",
    "        n_classes=10,\n",
    "        class_sep=2.5)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the classifcation data, training and test data sets, and define our hyperparameter\n",
    "grid. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = create_classification_data()\n",
    "# Split the dataset into train and test sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=1000)\n",
    "\n",
    "# Example parameters grid to tune from SGDClassifier\n",
    "parameter_grid = {\"alpha\": [1e-4, 1e-1, 1], \"epsilon\": [0.01, 0.1]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Sklearn to train the model\n",
    "\n",
    "Run this on a single node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "CPU times: user 1.98 s, sys: 171 ms, total: 2.15 s\n",
      "Wall time: 45.8 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=SGDClassifier(), n_jobs=-1,\n",
       "             param_grid={'alpha': [0.0001, 0.1, 1], 'epsilon': [0.01, 0.1]},\n",
       "             verbose=True)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# n_jobs=-1 enables use of all cores\n",
    "sklearn_search = GridSearchCV(SGDClassifier(), parameter_grid, n_jobs=-1, verbose=True)\n",
    "sklearn_search.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters found were:  {'alpha': 1, 'epsilon': 0.01}\n"
     ]
    }
   ],
   "source": [
    "print(\"Best hyperparameters found were: \", sklearn_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Ray's Tune's drop-in replacement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And from here, we proceed just like how we would in scikit-learn’s interface!\n",
    "\n",
    "The `SGDClassifier` has a `partial_fit` API, which enables it to stop fitting to the data for a certain hyperparameter configuration. If the estimator does not support early stopping, we would fall back to a parallel grid search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the setup here is exactly how you would do it for scikit-learn, except we replace `GridSearchCV` with `TuneGridSearchCV`. Now, let's try fitting a model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start Ray on the Databricks Cluster\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-16 16:04:28,047\tINFO services.py:1412 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8266\u001b[39m\u001b[22m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': '127.0.0.1',\n",
       " 'raylet_ip_address': '127.0.0.1',\n",
       " 'redis_address': None,\n",
       " 'object_store_address': '/tmp/ray/session_2022-03-16_16-04-25_531343_58799/sockets/plasma_store',\n",
       " 'raylet_socket_name': '/tmp/ray/session_2022-03-16_16-04-25_531343_58799/sockets/raylet',\n",
       " 'webui_url': '127.0.0.1:8266',\n",
       " 'session_dir': '/tmp/ray/session_2022-03-16_16-04-25_531343_58799',\n",
       " 'metrics_export_port': 62488,\n",
       " 'gcs_address': '127.0.0.1:57988',\n",
       " 'address': '127.0.0.1:57988',\n",
       " 'node_id': 'fe79d325015f88c835080744ae0c9f6ea9bca6a4963632d430ee0945'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ray.util.spark import setup_ray_cluster, shutdown_ray_cluster\n",
    "import ray \n",
    "\n",
    "setup_ray_cluster(\n",
    "  num_worker_nodes=2,\n",
    "  num_cpus_per_node=4,\n",
    "  collect_log_to_path=\"/dbfs/path/to/ray_collected_logs\"\n",
    ")\n",
    "ray.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the slight differences we introduced below:\n",
    "\n",
    " * an `early_stopping`, and\n",
    " * a specification of `max_iters` parameter\n",
    "\n",
    "The ``early_stopping`` parameter allows us to terminate unpromising configurations. If ``early_stopping=True``, ``TuneGridSearchCV`` will default to using Tune's [ASHAScheduler](https://docs.ray.io/en/latest/tune/api_docs/schedulers.html#tune-scheduler-hyperband). You can pass in a custom algorithm - see Tune's documentation on [schedulers](https://docs.ray.io/en/latest/tune/api_docs/schedulers.html#tune-schedulers) for a full list to choose from.\n",
    "\n",
    "``max_iters`` is the maximum number of iterations a given hyperparameter set could run for; it may run for fewer iterations if it is early stopped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-03-16 16:04:43 (running for 00:00:12.37)<br>Memory usage on this node: 16.9/32.0 GiB<br>Using AsyncHyperBand: num_stopped=4\n",
       "Bracket: Iter 4.000: 0.8637999999999999 | Iter 1.000: 0.85515<br>Resources requested: 0/12 CPUs, 0/0 GPUs, 0.0/12.81 GiB heap, 0.0/2.0 GiB objects<br>Current best trial: 7006d_00001 with average_test_score=0.8637 and parameters={'early_stopping': True, 'early_stop_type': <EarlyStopping.PARTIAL_FIT: 1>, 'X_id': ObjectRef(00ffffffffffffffffffffffffffffffffffffff0100000001000000), 'y_id': ObjectRef(00ffffffffffffffffffffffffffffffffffffff0100000002000000), 'groups': None, 'cv': StratifiedKFold(n_splits=5, random_state=None, shuffle=False), 'fit_params': {}, 'scoring': {'score': <function _passthrough_scorer at 0x7f974085c790>}, 'max_iters': 10, 'return_train_score': False, 'n_jobs': 1, 'metric_name': 'average_test_score', 'alpha': 0.1, 'epsilon': 0.01, 'estimator_ids': [ObjectRef(00ffffffffffffffffffffffffffffffffffffff0100000003000000), ObjectRef(00ffffffffffffffffffffffffffffffffffffff0100000004000000), ObjectRef(00ffffffffffffffffffffffffffffffffffffff0100000005000000), ObjectRef(00ffffffffffffffffffffffffffffffffffffff0100000006000000), ObjectRef(00ffffffffffffffffffffffffffffffffffffff0100000007000000)]}<br>Result logdir: /Users/jules/ray_results/AcademyTraining<br>Number of trials: 6/6 (6 TERMINATED)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-16 16:04:44,003\tINFO tune.py:639 -- Total run time: 12.94 seconds (12.35 seconds for the tuning loop).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.58 s, sys: 501 ms, total: 4.08 s\n",
      "Wall time: 15.9 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TuneGridSearchCV(early_stopping=<ray.tune.schedulers.async_hyperband.AsyncHyperBandScheduler object at 0x7f971255f520>,\n",
       "                 estimator=SGDClassifier(),\n",
       "                 loggers=[<class 'ray.tune.logger.JsonLogger'>,\n",
       "                          <class 'ray.tune.logger.CSVLogger'>],\n",
       "                 max_iters=10, mode='max', n_jobs=-1, name='AcademyTraining',\n",
       "                 param_grid={'alpha': [0.0001, 0.1, 1], 'epsilon': [0.01, 0.1]},\n",
       "                 scoring={'score': <function _passthrough_scorer at 0x7f974085c790>},\n",
       "                 sk_n_jobs=1, verbose=1)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "tune_search = TuneGridSearchCV(\n",
    "    SGDClassifier(), parameter_grid, early_stopping=True, \n",
    "    max_iters=10, name=\"AcademyTraining\", verbose=1)\n",
    "tune_search.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters found were:  {'alpha': 0.1, 'epsilon': 0.01}\n"
     ]
    }
   ],
   "source": [
    "print(\"Best hyperparameters found were: \", tune_search.best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Using Bayesian Optimization\n",
    "\n",
    "In addition to the grid search interface, tune-sklearn also provides an interface, `TuneSearchCV`, for sampling from **distributions of hyperparameters**.\n",
    "\n",
    "In addition, you can easily enable Bayesian optimization over the distributions in only 2 lines of code:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-03-16 16:05:07 (running for 00:00:03.05)<br>Memory usage on this node: 16.6/32.0 GiB<br>Using AsyncHyperBand: num_stopped=0\n",
       "Bracket: Iter 4.000: 0.9314556716995741 | Iter 1.000: 0.8802990708478513<br>Resources requested: 0/12 CPUs, 0/0 GPUs, 0.0/12.81 GiB heap, 0.0/2.0 GiB objects<br>Result logdir: /Users/jules/ray_results/_Trainable_2022-03-16_16-05-04<br>Number of trials: 3/3 (3 TERMINATED)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-16 16:05:08,025\tINFO tune.py:639 -- Total run time: 3.17 seconds (3.04 seconds for the tuning loop).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 310 ms, sys: 65 ms, total: 375 ms\n",
      "Wall time: 3.28 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TuneSearchCV(early_stopping=<ray.tune.schedulers.async_hyperband.AsyncHyperBandScheduler object at 0x7f97125f5190>,\n",
       "             estimator=SGDClassifier(),\n",
       "             loggers=[<class 'ray.tune.logger.JsonLogger'>,\n",
       "                      <class 'ray.tune.logger.CSVLogger'>],\n",
       "             max_iters=10, mode='max', n_jobs=-1, n_trials=3,\n",
       "             param_distributions={'alpha': (0.0001, 1), 'epsilon': (0.01, 0.1)},\n",
       "             scoring={'score': <function _passthrough_scorer at 0x7f974085c790>},\n",
       "             search_optimization='bayesian', sk_n_jobs=1, verbose=1)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "digits = datasets.load_digits()\n",
    "x = digits.data\n",
    "y = digits.target\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=.2)\n",
    "\n",
    "clf = SGDClassifier()\n",
    "parameter_grid = {\"alpha\": (1e-4, 1), \"epsilon\": (0.01, 0.1)}\n",
    "\n",
    "bayopt_tune_search = TuneSearchCV(\n",
    "    clf,\n",
    "    parameter_grid,\n",
    "    search_optimization=\"bayesian\",\n",
    "    n_trials=3,\n",
    "    early_stopping=True,\n",
    "    max_iters=10,\n",
    "    verbose=1,\n",
    ")\n",
    "bayopt_tune_search.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters found were:  {'alpha': 0.5011315798901402, 'epsilon': 0.04885893039770795}\n"
     ]
    }
   ],
   "source": [
    "print(\"Best hyperparameters found were: \", bayopt_tune_search.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutdown_ray_cluster()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excercise\n",
    "\n",
    " * Try increasing the `n_samples` to 110K and `test_size=10000.` \n",
    " \n",
    " Run end-to-end. If the normal scikit-learn takes too long, stop it and continue with Ray's version.\n",
    " Do you see the difference in execution time?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
