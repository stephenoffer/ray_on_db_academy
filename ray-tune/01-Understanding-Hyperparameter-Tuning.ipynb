{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ray Tune - Understanding Hyperparameter Tuning Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many of the [Ray RLlib](../ray-rllib/00-Ray-RLlib-Overview.ipynb) lessons used [Ray Tune](http://tune.io) to train policies. This meant we trained _parameters_ that defined the policies. Now we'll learn that Ray Tune was actually designed to determine the best _hyperparameters_ for the problem, before training to determine the _parameters_.\n",
    "\n",
    "This lesson introduces the concepts of _Hyperparameter Tuning or Optimization_ (HPO) and works through a nontrivial example using Tune. \n",
    "\n",
    "See also the [Hyperparameter Tuning References](References-Hyperparameter-Tuning.ipynb) notebook and the [Tune documentation](http://tune.io), in particular, the [API reference](https://docs.ray.io/en/latest/tune/api_docs/overview.html). \n",
    "\n",
    "A [Ray Summit Connect](https://anyscale.com/blog/videos-and-slides-for-the-second-ray-summit-connect-june-17-2020/) talk by the creator of Tune, Richard Liaw, provides an excellent overview of the challenges of hyperparameter tuning and how Tune addresses these challenges. Another recent webinar [Fast and efficient hyperparameter tuning with Ray Tune](https://www.anyscale.com/events/2021/10/20/fast-scalable-hyperparameter-tuning-ray-tune) complements the discussion here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What Are Hyperparameters?\n",
    "\n",
    "In _supervised learning_, we train a model with labeled data so the model can properly label new data values. Everything about the model is defined by a set of _parameters_, such as the weights in a linear regression. \n",
    "\n",
    "In contrast, the _hyperparameters_<sup>1</sup> define structural details about the kind of model itself, like whether or not we are using a linear regression or what architecture is best for a neural network, etc. Other quantities considered hyperparameters include learning rates, discount rates, etc. If we want our training process and resulting model to work well, we first need to determine the optimal or near-optimal set of hyperparameters.\n",
    "\n",
    "How do we determine the optimal hyperparameters? The most straightfoward approach is to perform a loop where we pick a candidate set of values from some reasonably inclusive list of possible values, train a model, compare the results achieved with previous loop iterations, and pick the set that performed best. This process is called _Hyperparameter Tuning_ or _Optimization_ (HPO).\n",
    "\n",
    "This simple algorithm can quickly become very expensive, however. Training a single neural networks can be compute intensive and the space of all possible architectures is huge. Hence, much of the research in hyperparameter tuning, especially for neural networks, focuses on ways to optimize HPO, such as early stopping and pruning the search space when some combinations appear to perform poorly.\n",
    "\n",
    "1. _Hyperparameter_ is often spelled _hyper parameter_ or _hyper-parameter_, but we'll use the spelling with no space or dash.\n",
    "\n",
    "![](../images/tune/what-are-hyperparameters.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Simple Example: $k$-Means \n",
    "\n",
    "Let's start with a very simple example of HPO, finding $k$ in $k$-means. \n",
    "\n",
    "The $k$-means algorithm finds clusters in a data set. It's a canonical example of _unsupervised learning_, where information is extracted from a data set, rather than using labeled data to train a model for labelling new data, as in _supervised learning_. We won't discuss the algorithm details, but the essense of it involves a \"guess\" for the expected number of clusters, the $k$ value, then calculating $k$ centroids (the coordinates at the center), one per cluster, along with determining to which cluster each data point belongs. The details are in [$k$-means Wikipedia article](https://en.wikipedia.org/wiki/K-means_clustering). The following animation shows the algorithm in action for a two-dimensional data set where three clusters are evident.\n",
    "\n",
    "![K-Means Convergence](../images/tune/K-means_convergence.gif)\n",
    "\n",
    "(source: [Wikipedia](https://en.wikipedia.org/wiki/K-means_clustering). [Larger Image](https://en.wikipedia.org/wiki/K-means_clustering#/media/File:K-means_convergence.gif))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While it is easy to see the clusters in this two-dimensional data set, that won't be for arbitrary datasets, especially those with more than three dimensions. Hence, we should determine the best $k$ value by trying many values and picking the value that appears to be best. In this case, \"best\" would mean that we minimize the distances between the datapoints and centroids. \n",
    "\n",
    "With just one hyperparameter, this problem is comparatively simple and brute force calculations to find the optimal $k$ is usually good enough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HPO for Neural Networks\n",
    "\n",
    "Where HPO really becomes a challenge is finding the right neural network architecture for your problem. Why are neural networks a challenge? Consider this image of a typical architecture:\n",
    "\n",
    "![Typical Neural Network](../images/tune/hpo-neural-network-example.png)\n",
    "\n",
    "Every number you see is a hyperparameter! So are the decisions about how many layers to have, what kind of layer to use for each layer, etc. The space of possible hyperparameters is enormous, too big to explore naively.\n",
    "\n",
    "So called _neural architecture search_ (NAS) has become a research field in its own right, along with general research in optimizing HPO. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Ray Tune\n",
    "\n",
    "[Ray Tune](http://tune.io) is the Ray-based native library for hyperparameter tuning. Tune makes it nearly as easy to run distributed, parallelized HPO as it is to run trials on a single machine manually, one after the other. \n",
    "\n",
    "Tune is built as an extensible, pluggable framework, with built-in integrations for many frameworks, including [OpenAI Gym environments](https://gym.openai.com/envs/), [PyTorch](https://pytorch.org), [TensorFlow](http://tensorflow.org), and recently, [sci-kit learn](https://scikit-learn.org/stable/) (see [this recent blog post](https://medium.com/distributed-computing-with-ray/gridsearchcv-2-0-new-and-improved-ee56644cbabf)).\n",
    "\n",
    "## How Tune Works\n",
    "\n",
    "Before we get into using Tune, let's understand the some definitions, terms, and components. With this understanding, you will get an insight into what happens when you use Tune to search your hyperparameter space and optimize your process to select the best, optimized model.\n",
    "\n",
    "## Definitions\n",
    "\n",
    "After a short preview of the [Tune basic steps and concepts](01-Ray-Tune-Warmup.ipynb) in our warm up tutorial, let's get an intuition of what those terms and steps mean.  \n",
    "\n",
    "#### Trainable\n",
    "This is your training function, with an objective function. As [trainable](https://docs.ray.io/en/latest/tune/api_docs/trainable.html?highlight=trainable#ray.tune.Trainable), it's one of the argument to `tune.run(...)` method. Tune offers two iterface APIs for trainable: functional and class.\n",
    "\n",
    "\n",
    "#### Trial\n",
    "\n",
    "A trial is an execution or run of a logical representation of a single hyperparameter configuration. Each trial is associated with an instance of a Trainable. And a collection of trials comprise an experiment.\n",
    "\n",
    "#### Lifecycle of a trial¶\n",
    "A trial’s life cycle consists of 6 stages:\n",
    "\n",
    "Initialization (generation): A trial is first generated as a hyperparameter sample, and its parameters are configured according to what was provided in `tune.run` as part of the `config` arggument. Trials are then placed into a queue to be executed (with status PENDING).\n",
    "\n",
    "**PENDING**: A pending trial is a trial to be executed on the machine. Every trial is configured with resource values. Whenever the trial’s resource values are available, tune will run the trial (by starting a ray actor holding the config and the training function).\n",
    "\n",
    "**RUNNING**: A running trial is assigned a Ray Actor. There can be multiple running trials in parallel.\n",
    "\n",
    "**ERRORED**: If a running trial throws an exception, Tune will catch that exception and mark the trial as errored. Note that exceptions can be propagated from an actor to the main Tune driver process. If `max_retries` is set, Tune will set the trial back into “PENDING” and later start it from the last checkpoint.\n",
    "\n",
    "**TERMINATED**: A trial is terminated if it is stopped or finished by a Stopper/Scheduler. If using the Function API, the trial is also terminated when the function stops.\n",
    "\n",
    "**PAUSED**: A trial can be paused by a Trial scheduler. This means that the trial’s actor will be stopped too. A paused trial can later be resumed from the most recent checkpoint.\n",
    "\n",
    "\n",
    "#### Driver/worker process\n",
    "\n",
    "The driver process is the python process that calls `tune.run` (which calls ray.init() underneath the hood); therefore, you\n",
    "do not need to invoke `ray.init(...)` explicity. Tune does it for you during its inital run. The Tune's driver process runs on the node where you run your script (which calls `tune.run`), while Ray Tune trainable “actors” run on any node (either on the same node on multiple cores) or on worker nodes (with multiple cores on a distributed Ray cluster).\n",
    "\n",
    "#### Ray Actors\n",
    "\n",
    "Tune uses Ray Actors as worker node's processes to evaluate multiple Trainables in parallel.\n",
    "\n",
    "[Ray Actors](https://docs.ray.io/en/latest/actors.html#actor-guide) allow you to parallelize an instance of a class in Python. When you instantiate a class that is a Ray actor, Ray will start a instance of that class on a separate process either on the same machine (or another distributed machine, if running a Ray cluster). This actor can then asynchronously execute method calls and maintain its own internal state.\n",
    "\n",
    "### The execution of a trainable¶\n",
    "Tune uses Ray actors to parallelize the evaluation of multiple hyperparameter configurations. Each actor is a Python process that executes an instance of the user-provided Trainable. The definition of the user-provided Trainable will be [serialized via cloudpickle](https://docs.ray.io/en/latest/serialization.html#serialization-guide) and sent to each actor process. Each Ray actor will start an instance of the Trainable to be executed.\n",
    "\n",
    "If the Trainable is a class, it will be executed iteratively by calling train/step. After each invocation, the driver is notified that a “result dict” is ready. The driver will then pull the result via `ray.get`.\n",
    "\n",
    "If the trainable is a callable or a function, it will be executed on the Ray actor process on a separate execution thread. Whenever `tune.report` is called, the execution thread is paused and waits for the driver to pull a result. After pulling, the actor’s execution thread will automatically resume.\n",
    "\n",
    "The diagram below depicts how Tune launches trainables on the worker nodes as processes in which the the trainables are run. \n",
    "Each trial will have its own instance of a trainable, hence we parallelize trials and its respective configuration across cores on a worker. \n",
    "\n",
    "![](../images/ray_tune_report_launch_trainables.png)\n",
    "\n",
    "Whenever the trainble calls `tune.report`, the driver will pull the metrics via `ray.get`, as shown in the diagram below.\n",
    "\n",
    "![](../images/ray_tune_report_metrics.png)\n",
    "\n",
    "\n",
    "Tune also integrates implementations of many state-of-the-art [search algorithms](https://docs.ray.io/en/latest/tune/api_docs/suggestion.html) and [schedulers](https://docs.ray.io/en/latest/tune/api_docs/schedulers.html), so it is easy to optimize your HPO process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "shutdown_ray_cluster()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:3px solid gray\"> </hr>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
