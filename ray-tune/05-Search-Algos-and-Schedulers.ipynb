{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ray Tune - Search Algorithms and Schedulers\n",
    "\n",
    "This notebook introduces the concepts of search algorithms and schedulers which help optimize HPO. We'll see an example that combines the use of one search algorithm and one schedulers.\n",
    "\n",
    "The full set of search algorithms provided by Tune is documented [here](https://docs.ray.io/en/latest/tune/api_docs/suggestion.html), along with information about implementing your own. The full set of schedulers provided is documented [here](https://docs.ray.io/en/latest/tune/api_docs/schedulers.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to install a few libraries. We'll explain what they are below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hpbandster in /usr/local/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages (0.7.4)\n",
      "Requirement already satisfied: ConfigSpace in /usr/local/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages (0.4.20)\n",
      "Requirement already satisfied: Pyro4 in /usr/local/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages (from hpbandster) (4.81)\n",
      "Requirement already satisfied: netifaces in /usr/local/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages (from hpbandster) (0.11.0)\n",
      "Requirement already satisfied: statsmodels in /usr/local/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages (from hpbandster) (0.13.1)\n",
      "Requirement already satisfied: serpent in /usr/local/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages (from hpbandster) (1.40)\n",
      "Requirement already satisfied: numpy in /usr/local/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages (from hpbandster) (1.21.4)\n",
      "Requirement already satisfied: scipy in /usr/local/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages (from hpbandster) (1.7.2)\n",
      "Requirement already satisfied: cython in /usr/local/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages (from ConfigSpace) (0.29.24)\n",
      "Requirement already satisfied: pyparsing in /usr/local/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages (from ConfigSpace) (3.0.6)\n",
      "Requirement already satisfied: patsy>=0.5.2 in /usr/local/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages (from statsmodels->hpbandster) (0.5.2)\n",
      "Requirement already satisfied: pandas>=0.25 in /usr/local/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages (from statsmodels->hpbandster) (1.3.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages (from pandas>=0.25->statsmodels->hpbandster) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/local/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages (from pandas>=0.25->statsmodels->hpbandster) (2021.3)\n",
      "Requirement already satisfied: six in /usr/local/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages (from patsy>=0.5.2->statsmodels->hpbandster) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install hpbandster ConfigSpace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.7.12\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **NOTE:** If you are see **Python 3.6** in the output from the previous cell, run remove the `#` in the following cell and run it. This will fix a dependency bug needed for this notebook.\n",
    "> \n",
    "> Afterwards, **restart the kernel for this notebook**, using the circular error in the tool bar. After that, proceed with the rest of the notebook. \n",
    "> \n",
    "> If you have **Python 3.7** or later, skip these steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install statsmodels -U --pre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About Search Algorithms\n",
    "\n",
    "Tune integrates many [open source optimization libraries](https://docs.ray.io/en/latest/tune/api_docs/suggestion.html), each of which defines the parameter search space in its own way. Hence, you should read the corresponding documentation for an algorithm to understand the particular details of using it.\n",
    "\n",
    "Some of the search algorithms supported include the following:\n",
    "\n",
    "* [Bayesian Optimization](https://github.com/fmfn/BayesianOptimization): This constrained global optimization process builds upon bayesian inference and gaussian processes. It attempts to find the maximum value of an unknown function in as few iterations as possible. This is a good technique for optimization of high cost functions.\n",
    "* [BOHB (Bayesian Optimization HyperBand](https://github.com/automl/HpBandSter): An algorithm that both terminates bad trials and also uses Bayesian Optimization to improve the hyperparameter search. It is backed by the [HpBandSter](https://docs.ray.io/en/latest/tune/api_docs/schedulers.html#tune-scheduler-bohb) library. BOHB is intended to be paired with a specific scheduler class: [HyperBandForBOHB](https://docs.ray.io/en/latest/tune/api_docs/schedulers.html#tune-scheduler-bohb).\n",
    "* [HyperOpt](http://hyperopt.github.io/hyperopt): A Python library for serial and parallel optimization over awkward search spaces, which may include real-valued, discrete, and conditional dimensions.\n",
    "* [Nevergrad](https://github.com/facebookresearch/nevergrad): HPO without computing gradients.\n",
    "\n",
    "These and other algorithms are described in the [documentation](https://docs.ray.io/en/latest/tune/api_docs/suggestion.html).\n",
    "\n",
    "A limitation of search algorithms used by themselves is they can't affect or stop training processes, for example early stopping of trials that are performing poorly. The schedulers can do this, so it's common to use a compatible search algorithm with a scheduler, as we'll show in the first example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About Schedulers\n",
    "\n",
    "Tune includes distributed implementations of several early-stopping algorithms, including the following:\n",
    "\n",
    "* [Median Stopping Rule](https://research.google.com/pubs/pub46180.html): It applies the simple rule that a trial is aborted if the results are trending below the median of the previous trials.\n",
    "* [HyperBand](https://arxiv.org/abs/1603.06560): It structures search as an _infinite-armed, stochastic, exploration-only, multi-armed bandit_. See the [Multi-Armed Bandits lessons](../ray-rllib/multi-armed-bandits/00-Multi-Armed-Bandits-Overview.ipynb) for information on these concepts. The infinite arms correspond to the tunable parameters. Trying values stochastically ensures quick exploration of the parameter space. Exploration-only is desirable because for HPO, we aren't interested in _exploiting_ parameter combinations we've already tried (the usual case when using MABs where rewards are the goal). Intead, we need to explore as many new parameter combinations as possible.\n",
    "* [ASHA](https://openreview.net/forum?id=S1Y7OOlRZ). This is an aynchronous version of HyperBand that improves on the latter. Hence it is recommended over the original HyperBand implementation. \n",
    "\n",
    "Tune also includes a distributed implementation of [Population Based Training (PBT)](https://deepmind.com/blog/population-based-training-neural-networks). When the PBT scheduler is enabled, each trial variant is treated as a member of the _population_. Periodically, top-performing trials are checkpointed, which means your [`tune.Trainable`](https://docs.ray.io/en/latest/tune/api_docs/trainable.html#tune-trainable) object (e.g., the `TrainMNist` class we used in the previous exercise) has to support save and restore. \n",
    "\n",
    "Low-performing trials clone the checkpoints of top performers and perturb the configurations in the hope of discovering an even better variation. PBT trains a group of models (or RLlib agents) in parallel. So, unlike other hyperparameter search algorithms, PBT mutates hyperparameters during training time. This enables very fast hyperparameter discovery and also automatically discovers good [annealing](https://en.wikipedia.org/wiki/Simulated_annealing) schedules.\n",
    "\n",
    "See the [Tune schedulers](https://docs.ray.io/en/latest/tune/api_docs/schedulers.html) for a complete list and descriptions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples\n",
    "\n",
    "Let's initialize Ray as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray import tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.util.spark import setup_ray_cluster, shutdown_ray_cluster\n",
    "\n",
    "setup_ray_cluster(\n",
    "  num_worker_nodes=2,\n",
    "  num_cpus_per_node=4,\n",
    "  collect_log_to_path=\"/dbfs/path/to/ray_collected_logs\"\n",
    ")\n",
    "ray.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BOHB\n",
    "\n",
    "BOHB (Bayesian Optimization HyperBand) is an algorithm that both terminates bad trials and also uses Bayesian Optimization to improve the hyperparameter search. The [Tune implementation](https://docs.ray.io/en/latest/tune/api_docs/suggestion.html#bohb-tune-suggest-bohb-tunebohb) is backed by the [HpBandSter library](https://github.com/automl/HpBandSter), which we must install, along with [ConfigSpace](https://automl.github.io/HpBandSter/build/html/quickstart.html#searchspace), which is used to define the search space specification:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use BOHB with the scheduler [HyperBandForBOHB](https://docs.ray.io/en/latest/tune/api_docs/schedulers.html#bohb-tune-schedulers-hyperbandforbohb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try it. We'll use the same MNIST example from the previous lesson, but this time, we'll import the code from a file in this directory, `mnist.py`. Note that the implementation of `TrainMNIST` in the file has enhancements not present in the previous lesson, such as methods to support saving and restoring checkpoints, which are required to be used here. See the code comments for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mnist import ConvNet, TrainMNIST, EPOCH_SIZE, TEST_SIZE, DATA_ROOT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import and configure the `ConfigSpace` object we need for the search algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ConfigSpace as CS\n",
    "from ray.tune.schedulers.hb_bohb import HyperBandForBOHB\n",
    "from ray.tune.suggest.bohb import TuneBOHB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_space = CS.ConfigurationSpace()\n",
    "\n",
    "# There are also UniformIntegerHyperparameter and UniformFloatHyperparameter\n",
    "# objects for defining integer and float ranges, respectively. For example:\n",
    "# config_space.add_hyperparameter(\n",
    "#     CS.UniformIntegerHyperparameter('foo', lower=0, upper=100))\n",
    "\n",
    "config_space.add_hyperparameter(\n",
    "    CS.CategoricalHyperparameter('lr', choices=[0.001, 0.01, 0.1]))\n",
    "config_space.add_hyperparameter(\n",
    "    CS.CategoricalHyperparameter('momentum', choices=[0.001, 0.01, 0.1, 0.9]))\n",
    "\n",
    "config_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_metrics = dict(metric=\"mean_accuracy\", mode=\"max\")\n",
    "\n",
    "search_algorithm = TuneBOHB(config_space, **experiment_metrics)\n",
    "search_alg = tune.suggest.ConcurrencyLimiter(search_algorithm, max_concurrent=4)\n",
    "scheduler = HyperBandForBOHB(\n",
    "    time_attr='training_iteration',\n",
    "    reduction_factor=4,\n",
    "    max_t=200,\n",
    "    **experiment_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Through experimentation, we determined that `max_t=200` is necessary to get good results. For the smallest learning rate and momentum values, it takes longer for training to converge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "analysis = tune.run(TrainMNIST, \n",
    "    scheduler=scheduler, \n",
    "    search_alg=search_alg, \n",
    "    num_samples=12,                           # Force it try all 12 combinations\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = analysis.stats()\n",
    "secs = stats[\"timestamp\"] - stats[\"start_time\"]\n",
    "print(f'{secs:7.2f} seconds, {secs/60.0:7.2f} minutes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best config: \", analysis.get_best_config(metric=\"mean_accuracy\", mode=\"max\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis.dataframe().sort_values('mean_accuracy', ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis.dataframe()[['mean_accuracy', 'config/lr', 'config/momentum']].sort_values('mean_accuracy', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The runs in the previous lesson, for the class-based and the function-based Tune APIs, took between 12 and 20 seconds (on my machine), but we only trained for 20 iterations, where as here we went for 100 iterations. That also accounts for the different results, notably that a much smaller momentum value `0.01` and `0.1` perform best here, while for the the previous lesson `0.9` performed best. This is because a smaller momentum value will result in longer training times required, but more fine-tuned iterating to the optimal result, so more training iterations will favor a smaller momentum value. Still, the mean accuracies among the top three or four combinations are quite close."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise - Population Base Training\n",
    "\n",
    "Read the [documentation]() on _population based training_ to understand what it is doing. The next cell configures a PBT scheduler and defines other things you'll need. \n",
    "\n",
    "See also the discussion for the results in the [solutions](solutions/03-Search-Algos-and-Schedulers-Solutions.ipynb).\n",
    "\n",
    "> **NOTE:** For a more complete example using MNIST and PyTorch, see [this example code](https://github.com/ray-project/ray/blob/master/python/ray/tune/examples/mnist_pytorch_lightning.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.tune.schedulers import PopulationBasedTraining\n",
    "\n",
    "pbt_scheduler = PopulationBasedTraining(\n",
    "        time_attr='training_iteration',\n",
    "        perturbation_interval=10,  # Every N time_attr units, \"perturb\" the parameters.\n",
    "        hyperparam_mutations={\n",
    "            \"lr\": [0.001, 0.01, 0.1],\n",
    "            \"momentum\": [0.001, 0.01, 0.1, 0.9]\n",
    "        },\n",
    "        **experiment_metrics)\n",
    "\n",
    "# Note: This appears to be needed to avoid a \"key error\", but in fact these values won't change\n",
    "# in the analysis.dataframe() object, even though they will be tuned by the PBT scheduler.\n",
    "# So when you look at the analysis.dataframe(), look at the `experiment_tag` to see the actual values!\n",
    "config = {\n",
    "    \"lr\": 0.001,            # Use the lowest values from the previous definition\n",
    "    \"momentum\": 0.001\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run the the following cell, modified from above, which makes these changes:\n",
    "1. Uses the new scheduler.\n",
    "2. Removes the search_alg argument.\n",
    "3. Adds the `config` argument.\n",
    "4. Don't allow it to keep going past `0.97` accuracy for `600` iterations.\n",
    "5. Use `1` for the `verbose` argument to reduce the \"noise\".\n",
    "\n",
    "Then run it. \n",
    "\n",
    "> **WARNING:** This will run for a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis = tune.run(TrainMNIST, \n",
    "    scheduler=pbt_scheduler, \n",
    "    config=config,\n",
    "    stop={\"mean_accuracy\": 0.97, \"training_iteration\": 600},\n",
    "    num_samples=8,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "stats = analysis.stats()\n",
    "secs = stats[\"timestamp\"] - stats[\"start_time\"]\n",
    "print(f'{secs:7.2f} seconds, {secs/60.0:7.2f} minutes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the `analysis` data of interest, as done previously. (You might want to focus on other columns in the dataframe.) How well does PBT work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutdown_ray_cluster()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
