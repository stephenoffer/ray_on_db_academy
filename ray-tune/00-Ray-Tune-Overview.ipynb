{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ray Tune - Overview\n",
    "\n",
    "## About This Tutorial\n",
    "\n",
    "This tutorial introduces _hyperparameter tuning_, often called _hyperparameter optimization_ for which we'll use the acronym _HPO_ (since \"hyperparameter\" is often spelled \"hyper parameter\"). \n",
    "\n",
    "In particular this tutorial introduces [Ray Tune](http://tune.io), Ray's comprehensive HPO library.\n",
    "\n",
    "![Ray Tune](../images/RayTune.png)\n",
    "\n",
    "See the instructions in the [README](../README.md) for setting up your environment to use this tutorial.\n",
    "\n",
    "Go [here](../Overview.ipynb) for an overview of all tutorials."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|     | Lesson | Description |\n",
    "| :-- | :----- | :---------- |\n",
    "| 00  | [Ray Tune Overview](00-Ray-Tune-Overview.ipynb) | Overview of this tutorial. |\n",
    "| 01  | [Understanding Hyperparameter Tuning](01-Understanding-Hyperparameter-Tuning.ipynb) | An explanation of hyperparameters vs. parameters. Also, some definitions and terms, along with diagrams showing the lifecycle of a trial. |\n",
    "| 02  | [Ray Tune Warm Up](02-Ray-Tune-Warmup.ipynb) | Getting Started of Ray Tune Steps 1-2. |\n",
    "| 03  | [Ray Tune with Sklearn](03-Ray-Tune-with-Sklearn.ipynb) | More exploration of the Tune API, using Tune's replacements for GridSearchCV and RandomizedSearchCV example. |\n",
    "| 04  | [Ray Tune with MNIST](04-Ray-Tune-with-MNIST.ipynb) | More exploration of the Tune API, using an MNIST example. |\n",
    "| 05  | [Search Algos and Schedulers](05-Search-Algos-and-Schedulers.ipynb) | Understanding the concepts of search algorithms and schedulers, again using an MNIST example. |\n",
    "| 06  | [End-to-End XGBoost with Ray Tune](06-Ray-Tune-and-XGBoost.ipynb) | An end-to-end example of using XGBoost and Ray Tune |\n",
    "| 07  | [Ray Tune with TuneSearchCV and XGBoost](06-Ray-Tune-with-TuneSearchCV.ipynb) | A real life example of using Tune's drop-in replacements for HPO with XGBoost. With only few lines you can replace your existing sci-kit learn hyperparameter search with Tune's distributed HPO |\n",
    "| 08    | [Hyperparameter Tuning References](References-Hyperparameter-Tuning.ipynb) | Read through these references for hyperparameter tuning. |\n",
    "\n",
    "In addition, exercise solutions for this tutorial can be found in the `solutions` directory.\n",
    "\n",
    "For other, earlier tutorials that use Tune,  \n",
    " * [code examples](https://github.com/ray-project/tune-sklearn/tree/master/examples) from the Ray GitHub\n",
    " * [tune examples](https://docs.ray.io/en/latest/tune/examples/index.html) in the Ray Tune documentation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
