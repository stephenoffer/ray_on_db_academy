{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29b25bca-0bfb-44de-b8f8-4ebbe4a427d2",
   "metadata": {},
   "source": [
    "# Ray Train - A Library for Distributed Deep Learning\n",
    "\n",
    "[Ray Train](https://docs.ray.io/en/latest/train/train.html) is a lightweight library for distributed deep learning. It provides thin wrappers around [PyTorch](https://pytorch.org), [TensorFlow](https://tensorflow.org), and [Horvod](https://horovod.ai/) native modules for data parallel training.\n",
    "\n",
    "> **NOTE**: Ray SGD is renamed to Ray Train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eda5885-0327-4fa6-b357-bb84cf2db6fe",
   "metadata": {},
   "source": [
    "### Introduction to Ray Train\n",
    "\n",
    "Ray Train is a library that aims to simplify distributed deep learning. As a library, Ray Train is built to abstract away the coordination/configuration setup of distributed deep learning frameworks such as [Pytorch Distributed](https://pytorch.org/tutorials/beginner/dist_overview.html) and [Tensorflow Distributed](https://www.tensorflow.org/guide/distributed_training), allowing users to only focus on implementing training logic for their respective framework. For example: \n",
    " * For Pytorch, Ray Train automatically handles the construction of the distributed process group.\n",
    " * For Tensorflow, Ray Train automatically handles the coordination of the `TF_CONFIG`. The current implementation assumes that the user will use a _MultiWorkerMirroredStrategy_, but this will change in the near future.\n",
    " * For Horovod, Ray Train automatically handles the construction of the Horovod runtime and [Rendezvous server](https://horovod.readthedocs.io/en/stable/_modules/horovod/ray/runner.html).\n",
    "\n",
    "Built for data scientists/ML practitioners, Ray Train has support for standard ML tools and features that practitioners love. For example:\n",
    " * Callbacks for early stopping, reducing costs and time for training\n",
    " * Checkpointing at regular intervals, allowing to restart for fault-tolerence\n",
    " * Integration with Tensorboard, Weights/Biases, and MLflow, providing extensibilty for experimentation and observation of runs\n",
    " * Jupyter notebooks, giving developers familiar development tools for iteration and experimentation\n",
    "\n",
    "More importantly, Ray Train integrates with the Ray Ecosystem. Distributed deep learning often comes with a lot of complexity, so you can:\n",
    " * Use [Ray Datasets](https://docs.ray.io/en/latest/data/dataset.html#datasets) with Ray Train to inject, handle or train on large amounts of data\n",
    " * Use [Ray Tune](https://docs.ray.io/en/latest/tune/index.html#tune-main) with Ray Train to leverage cutting edge hyperparameter techniques and distribute both your training and tuning\n",
    " * Use the [Ray cluster launcher](https://docs.ray.io/en/latest/cluster/cloud.html#cluster-cloud) to launch and leverage autoscaling or spot instance clusters to train your model at scale on any cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dfbcdf5-b82b-4440-bc90-eebd55aa29c6",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Ray Train Architecture and concepts\n",
    "\n",
    "<img src=\"https://docs.ray.io/en/latest/_images/train-arch.svg\" width=\"50%\" height=\"60%\"> \n",
    "\n",
    "**Trainer**: The Trainer is the main class that is exposed in the [Ray Train API](https://docs.ray.io/en/latest/train/api.html) that users will interact with. A user will pass in a function which defines the training logic. In our case, the trainin\n",
    "function is `train_func_distributed` with `configs` as its argument. The Trainer will create an Executor to run the distributed training. It will also will handle callbacks based on the results from the `BackendExecutor`. Read the Trainer [source here](https://github.com/ray-project/ray/blob/f1acabe9cf37d5d123017fb3f158c37fb36499a5/python/ray/train/trainer.py#L78).\n",
    "\n",
    "**BackendExecutor**: The executor is an interface that handles execution of distributed training. It creates an actor group and initializes in conjunction with a specific backend. Worker resources, number of workers, and placement strategy are passed to the `Worker Group.` Read the BackendExecutor [source here](https://github.com/ray-project/ray/blob/f1acabe9cf37d5d123017fb3f158c37fb36499a5/python/ray/train/backend.py#L102).\n",
    "\n",
    "**Backend**: A backend is used in conjunction with the `Executor` to initialize and manage framework-specific communication protocols. Each communication library (Torch, Horovod, TensorFlow, etc.) will have a separate backend and will take a specific configuration value. In the diagram, they are labelled as `XBackend`, `XConfig`, `YBackend`, and `YConfig` respectively. Read the Backend [source here](https://github.com/ray-project/ray/blob/f1acabe9cf37d5d123017fb3f158c37fb36499a5/python/ray/train/trainer.py#L64).\n",
    "\n",
    "**WorkerGroup**:The `WorkerGroup` is a generic utility class for managing a group of Ray Actors, regardless of the backend. Read WorkGroup [source here](https://github.com/ray-project/ray/blob/f1acabe9cf37d5d123017fb3f158c37fb36499a5/python/ray/train/worker_group.py#L84).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078041db-c545-4c19-ae96-4d8a7d90e83f",
   "metadata": {},
   "source": [
    "### Quick Start: Distributed training on multiple workers with PyTorch\n",
    "\n",
    "Let's work through a typical distributed PyTorch trainining example, where we only use Ray Train with multipler workper process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "591c4484-e98d-4fa6-a925-c8bd97c09afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "#from tqdm.notebook import tqdm_notebook\n",
    "from tqdm import tqdm\n",
    "\n",
    "# import from Ray \n",
    "from ray import train\n",
    "from ray.train import Trainer\n",
    "\n",
    "from ray.util.spark import setup_ray_cluster, shutdown_ray_cluster\n",
    "import ray \n",
    "\n",
    "setup_ray_cluster(\n",
    "  num_worker_nodes=2,\n",
    "  num_cpus_per_node=4,\n",
    "  collect_log_to_path=\"/dbfs/path/to/ray_collected_logs\"\n",
    ")\n",
    "ray.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5638c531-46da-464c-81c1-a4ca01cebc14",
   "metadata": {},
   "source": [
    "### Step 1. Define constants, input and output variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d7cd6a6-fb19-447b-a5da-eda064856cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_SAMPLES = 20             # our dataset for training\n",
    "INPUT_SIZE = 20              # inputs or neurons into the first layer\n",
    "LAYER_SIZE = 15              # inputs or neurons to the hidden layer\n",
    "OUTPUT_SIZE = 5              # outputs to the last layer\n",
    "\n",
    "# In this example we use a randomly generated dataset.\n",
    "input = torch.randn(NUM_SAMPLES, INPUT_SIZE)         # In normal ML parlance, X\n",
    "labels = torch.randn(NUM_SAMPLES, OUTPUT_SIZE)       # In nmormal ML parlance, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ea37c5-3c9e-4ebe-9980-9b582ed4b7e3",
   "metadata": {},
   "source": [
    "### Step 2: Define a simple PyTorch neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9de1ed0-31f3-496a-9212-7188f4d41158",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.layer1 = nn.Linear(in_features=INPUT_SIZE, out_features=LAYER_SIZE)\n",
    "        # Our activation function\n",
    "        self.relu = nn.ReLU()           \n",
    "        self.layer2 = nn.Linear(in_features=LAYER_SIZE, out_features=OUTPUT_SIZE)\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.layer2(self.relu(self.layer1(input)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76570c70-22c8-4e31-a5da-1f63c3fd9b5d",
   "metadata": {},
   "source": [
    "### Step 3: Define our training function used by Ray Train\n",
    "Simple function that iterates over epochs and does standard PyTorch steps:\n",
    " * Invoke the callable model with input\n",
    " * Calculate the loss\n",
    " * Zero out the gradients\n",
    " * Do backward propogation\n",
    " * Optimize the step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d863d04-ca80-47c3-9168-1df1eb7abff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_func_distributed(config):\n",
    "   \n",
    "    model = NeuralNetwork()\n",
    "    model = train.torch.prepare_model(model, move_to_device=True)\n",
    "    loss_fn = nn.MSELoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "    # Iterate over the loop\n",
    "    epochs = config.get('NUM_EPOCHS', [20, 40, 60])\n",
    "    for epoch in epochs: \n",
    "        for e in tqdm(range(epoch)):\n",
    "            output = model(input)\n",
    "            loss = loss_fn(output, labels)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "            if e % epoch == 0:\n",
    "                print(f'epoch {epoch}, loss: {loss.item():.3f}')\n",
    "    # Return anything you want, here we just report back the pid on which this function\n",
    "    # runs\n",
    "    return os.getpid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d206f590-ba6a-4e86-ae7c-5e317ddc009e",
   "metadata": {},
   "source": [
    "### Step 4: Train the model\n",
    "\n",
    "We create the Trainer, the main class as shown in the above Ray Train architecture diagram. This in turn will connect to the Ray cluster, without us using `ray.init(...)`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9716697a-57f6-45b5-b18b-5ad885f24944",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-16 16:13:37,747\tINFO services.py:1412 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8266\u001b[39m\u001b[22m\n",
      "2022-03-16 16:13:40,251\tINFO trainer.py:199 -- Trainer logs will be logged in: /Users/jules/ray_results/train_2022-03-16_16-13-40\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=60843)\u001b[0m 2022-03-16 16:13:41,866\tINFO torch.py:66 -- Setting up process group for: env:// [rank=0, world_size=4]\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=60838)\u001b[0m 2022-03-16 16:13:41,867\tINFO torch.py:66 -- Setting up process group for: env:// [rank=1, world_size=4]\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=60841)\u001b[0m 2022-03-16 16:13:41,866\tINFO torch.py:66 -- Setting up process group for: env:// [rank=2, world_size=4]\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=60837)\u001b[0m 2022-03-16 16:13:41,866\tINFO torch.py:66 -- Setting up process group for: env:// [rank=3, world_size=4]\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=60843)\u001b[0m [W ProcessGroupGloo.cpp:707] Warning: Unable to resolve hostname to a (local) address. Using the loopback address as fallback. Manually set the network interface to bind to with GLOO_SOCKET_IFNAME. (function operator())\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=60838)\u001b[0m [W ProcessGroupGloo.cpp:707] Warning: Unable to resolve hostname to a (local) address. Using the loopback address as fallback. Manually set the network interface to bind to with GLOO_SOCKET_IFNAME. (function operator())\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=60841)\u001b[0m [W ProcessGroupGloo.cpp:707] Warning: Unable to resolve hostname to a (local) address. Using the loopback address as fallback. Manually set the network interface to bind to with GLOO_SOCKET_IFNAME. (function operator())2022-03-16 16:13:42,925\tINFO trainer.py:205 -- Run results will be logged in: /Users/jules/ray_results/train_2022-03-16_16-13-40/run_001\n",
      "\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=60837)\u001b[0m [W ProcessGroupGloo.cpp:707] Warning: Unable to resolve hostname to a (local) address. Using the loopback address as fallback. Manually set the network interface to bind to with GLOO_SOCKET_IFNAME. (function operator())\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=60843)\u001b[0m 2022-03-16 16:13:42,948\tINFO torch.py:244 -- Moving model to device: cpu\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=60843)\u001b[0m 2022-03-16 16:13:42,948\tINFO torch.py:247 -- Wrapping provided model in DDP.\n",
      "100%|██████████| 20/20 [00:00<00:00, 500.64it/s]\n",
      "  0%|          | 0/40 [00:00<?, ?it/s]0m \n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=60838)\u001b[0m 2022-03-16 16:13:42,948\tINFO torch.py:244 -- Moving model to device: cpu\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=60838)\u001b[0m 2022-03-16 16:13:42,948\tINFO torch.py:247 -- Wrapping provided model in DDP.\n",
      "100%|██████████| 20/20 [00:00<00:00, 452.18it/s]\n",
      "  0%|          | 0/40 [00:00<?, ?it/s]0m \n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=60841)\u001b[0m 2022-03-16 16:13:42,948\tINFO torch.py:244 -- Moving model to device: cpu\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=60841)\u001b[0m 2022-03-16 16:13:42,948\tINFO torch.py:247 -- Wrapping provided model in DDP.\n",
      "100%|██████████| 20/20 [00:00<00:00, 469.84it/s]\n",
      "  0%|          | 0/40 [00:00<?, ?it/s]0m \n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=60837)\u001b[0m 2022-03-16 16:13:42,948\tINFO torch.py:244 -- Moving model to device: cpu\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=60837)\u001b[0m 2022-03-16 16:13:42,949\tINFO torch.py:247 -- Wrapping provided model in DDP.\n",
      "100%|██████████| 20/20 [00:00<00:00, 484.38it/s]\n",
      "  0%|          | 0/40 [00:00<?, ?it/s]0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=60843)\u001b[0m epoch 20, loss: 1.313\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=60837)\u001b[0m epoch 20, loss: 1.313\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=60841)\u001b[0m epoch 20, loss: 1.313\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=60841)\u001b[0m epoch 40, loss: 0.897\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=60838)\u001b[0m epoch 20, loss: 1.313\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=60838)\u001b[0m epoch 40, loss: 0.897\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=60843)\u001b[0m epoch 40, loss: 0.897\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=60837)\u001b[0m epoch 40, loss: 0.897\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=60843)\u001b[0m epoch 60, loss: 0.506\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=60837)\u001b[0m epoch 60, loss: 0.506\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=60841)\u001b[0m epoch 60, loss: 0.506\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=60838)\u001b[0m epoch 60, loss: 0.506\n",
      "[60843, 60838, 60841, 60837]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 751.85it/s]\n",
      "  0%|          | 0/60 [00:00<?, ?it/s]0m \n",
      "100%|██████████| 40/40 [00:00<00:00, 752.69it/s]\n",
      "  0%|          | 0/60 [00:00<?, ?it/s]0m \n",
      "100%|██████████| 40/40 [00:00<00:00, 751.99it/s]\n",
      "  0%|          | 0/60 [00:00<?, ?it/s]0m \n",
      "100%|██████████| 40/40 [00:00<00:00, 753.07it/s]\n",
      "  0%|          | 0/60 [00:00<?, ?it/s]0m \n",
      "100%|██████████| 60/60 [00:00<00:00, 926.42it/s]\n",
      "100%|██████████| 60/60 [00:00<00:00, 926.29it/s]\n",
      "100%|██████████| 60/60 [00:00<00:00, 926.45it/s]\n",
      "100%|██████████| 60/60 [00:00<00:00, 926.31it/s]\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(backend='torch', num_workers=4)\n",
    "trainer.start()\n",
    "results = trainer.run(train_func_distributed, config={'NUM_EPOCHS': [20, 40, 60]})\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a645313-8401-4f34-bc47-b8569f1f64e8",
   "metadata": {},
   "source": [
    "### Excercises\n",
    "\n",
    "Have a go at this in your spare time and observe the results\n",
    "\n",
    " 1. Change the NUM_EPOCHS list to **[200, 400, 600]**\n",
    " 2. Do you see the loss approaching zero?\n",
    " 3. Try changing sample sizes. Do you need more epochs to train and minimize loss?\n",
    " 4. Try chaning the number of workers to 1/2 number of cores on your localhost or laptop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "439d011a-ae5e-4403-93b4-ff3cbfc68d5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=60843)\u001b[0m /usr/local/anaconda3/lib/python3.8/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=60843)\u001b[0m   warnings.warn('resource_tracker: There appear to be %d '\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=60838)\u001b[0m /usr/local/anaconda3/lib/python3.8/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=60838)\u001b[0m   warnings.warn('resource_tracker: There appear to be %d '\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=60841)\u001b[0m /usr/local/anaconda3/lib/python3.8/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=60841)\u001b[0m   warnings.warn('resource_tracker: There appear to be %d '\n"
     ]
    }
   ],
   "source": [
    "trainer.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d003643-1b29-478f-8bac-02a132125eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "shutdown_ray_cluster()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
